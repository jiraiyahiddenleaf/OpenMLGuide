# Deep Dive into Transformers, Neural Network & Backprop

Dive deep into Transformers, Neural Networks, and backpropagation. Explore cutting-edge AI, uncover insights, and empower your understanding of these transformative technologies.

<br/>
<div>
  <img src="/static/img/memes/backpropAndTransformersMeme.png" alt="backprop & Transformers Meme" />
</div>

### Articles

- [How Transformers Work](https://towardsdatascience.com/transformers-141e32e69591): Discover the mechanics of Transformers, the revolutionary neural network technology harnessed by industry leaders like OpenAI and DeepMind. Gain valuable insights into the inner workings of these AI giants and explore their transformative applications in the world of ML.

- [The illustrated transformer](https://jalammar.github.io/illustrated-transformer/) by Jay Alammar provides an in-depth technical examination of the transformer framework, offering detailed insights into its structure and functionality.

- [Yes you should understand backprop](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b) by Andrej Karpathy, depth post on backpropagation provides valuable insights into the intricacies of this crucial technique.

- [RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html): Explore Chip Huyen's insights into RLHF, a transformative approach that enhances the predictability and human-friendliness of LLMs. Gain valuable knowledge about this crucial aspect of systems like ChatGPT, shedding light on its significance and potential impact.

- [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/): This post provides an annotated paper implementation of ["Attention Is All You Need"](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf), presented in a line-by-line format. It reorganizes and omits certain sections from the original paper while incorporating comments, offering a fully functional implementation with available code in a concise format. Requires some knowledge of PyTorch.

### Explainers

- Explore the video article [Attention Is All You Need video](https://www.youtube.com/watch?v=XowwKOAWYoQ) a visual journey through the groundbreaking concepts introduced in the original paper. If the written article seems complex, this video provides an accessible and insightful way to grasp the key ideas behind the transformative Transformer architecture.

- [Neural Network course](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) by 3Blue1Brown, Delve into the intricate world of neural networks through captivating animations and intuitive explanations. Gain a deep understanding of this fundamental ML concept in a visually engaging and enlightening way.

- [Introduction to Transformers](https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2287s) by Andrej Karpathy. Since their groundbreaking introduction in 2017, transformers have transformed NLP and expanded into various domains of Deep Learning, including computer vision (CV), reinforcement learning (RL), Generative Adversarial Networks (GANs), Speech, and even Biology. They played a pivotal role in the development of powerful language models like GPT-3 and were instrumental in DeepMind's remarkable AlphaFold2 project, addressing protein folding challenges.

- [The backpropagation algorithm](https://www.youtube.com/watch?v=VCT1N0EsGj0) by Geoffrey Hinton delves into the fundamentals of backpropagation, a key algorithm in training neural networks. Hinton's teachings include insights on learning with the idea behind it, the role of hidden units, and learning through perturbing weights, discusses the concept of learning by using perturbations, providing valuable knowledge and techniques for training neural networks effectively.

- [Tensors for Neural Networks, Clearly Explained!!!](https://www.youtube.com/watch?v=L35fFDpwIM4) by StatQuest with Josh Starmer. Tensors are fundamental data structures in machine learning, representing multi-dimensional arrays that store and manipulate information, enabling the foundation for deep learning and neural network operations.


