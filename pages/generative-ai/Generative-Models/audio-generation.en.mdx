# Auido & Music Generation

Audio generation is the production of sound or music using AI algorithms. These algorithms can compose music, mimic voices, or generate sound effects, contributing to industries like music composition, voice synthesis, and audio production.

![AI music meme](/static/img/memes/aiMusicMeme.png)

### Courses

- [Hugging Face Audio course](https://huggingface.co/learn/audio-course/chapter0/introduction): This course equips learners with the ability to address diverse audio tasks like speech recognition, classification, and text-to-speech using transformers. It delves into audio data intricacies, explores different transformer architectures, and empowers participants to train their audio transformers by leveraging potent pre-trained models.

### Article & Papers

- [MuseNet](https://openai.com/research/musenet) (2019) is an AI system developed by OpenAI that can generate 4-minute musical compositions with 10 different instruments. It is a deep neural network that can combine styles from country to Mozart to the Beatles, and it can generate music in a wide range of genres.

- [Jukebox](https://openai.com/research/jukebox) (2020), an OpenAI generative music model, generates music based on genre, artist, and lyrics. It advances musical quality, but a gap with human-created music remains [(paper)](https://cdn.openai.com/papers/jukebox.pdf). You can listen to its music on [Jukebox Music](https://jukebox.openai.com/). They have also released the [source code](https://github.com/openai/jukebox/).

- [MusicLM: Generating Music From Text](https://google-research.github.io/seanet/musiclm/examples/) (2023) by Google Research generates high-fidelity music from text descriptions and melodies, transforming hummed melodies to match text captions. Google also released the [MusicCaps dataset](https://huggingface.co/datasets/google/MusicCaps). AudioLM, another framework, generates high-quality audio, maintaining speaker identity and prosody. Both systems are remarkable AI developments for music and audio generation [(paper)](https://arxiv.org/abs/2301.11325).

- [AudioLM: a Language Modeling Approach to Audio Generation](https://google-research.github.io/seanet/audiolm/examples/) (2023): AudioLM, a Google language model, transcribes spoken language in real-time, accommodating multiple speakers and background noise. It supports various languages and accents, with applications in speech recognition, translation, voice assistants, and accessibility. Google has shared the code on GitHub for further development. It's a notable advancement in NLP [(paper)](https://arxiv.org/pdf/2209.03143v2.pdf) [(blog post)](https://blog.research.google/2022/10/audiolm-language-modeling-approach-to.html).

- [AudioCraft](https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/) (2023), a Meta AI tool, includes [MusicGen](https://huggingface.co/spaces/facebook/MusicGen), [AudioGen](https://felixkreuk.github.io/audiogen/), and [EnCodec](https://ai.meta.com/blog/ai-powered-audio-compression-technique/) models. MusicGen generates music, AudioGen creates audio from text inputs, and EnCodec improves music quality. It simplifies generative audio models and offers open-source [code](https://github.com/facebookresearch/audiocraft), advancing AI audio and music generation for faster feedback in early prototyping.



### Reference

- [Camenduru's Audio ML Papers](https://github.com/camenduru#-audio-ml-papers):  The GitHub repositories encompass audio generation, music captioning, voice conversion, text-to-speech, and more. Additionally, this repository offers substantial insights into audio generation, fostering a rich learning experience.









