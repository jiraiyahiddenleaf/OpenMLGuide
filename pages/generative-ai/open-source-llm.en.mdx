# Open-source LLMs

import { Callout, FileTree } from "nextra-theme-docs";

Open-source Large Language Models provide a versatile foundation for AI development. These models can be fine-tuned for specialized tasks, such as prompt generation or medical applications, offering flexibility and innovation opportunities across various domains, enhancing natural language processing capabilities.

<br />
![open-source LLMs](/static/img/osLLm.png)

## Models

| Models                                              | Developed by                                     | Parameter                                                                                                                                                                                      | Description                                                                                                                                                                                                                                                                                                                                                                                         |
| --------------------------------------------------- | ------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Mistral](https://mistral.ai/product/)              | [Mistral AI](https://mistral.ai/)                | [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1), [Mistral-7B-Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)                                               | Mistral-7B-v0.1 is a small, yet powerful model adaptable to many use-cases. It outperforms Llama 2 13B on all benchmark assessments, possesses inherent natural coding capabilities, and 8k sequence length. Released under the Apache 2.0 license, it has been designed for effortless deployment on a wide range of cloud platforms, ensuring accessibility and convenience for users.            |
| [Falcon](https://falconllm.tii.ae/)                 | [TII](https://falconllm.tii.ae/index.html)       | [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b), [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b), [Falcon-180B](https://huggingface.co/tiiuae/falcon-180B)                         | The Falcon LLM has garnered acclaim as an advanced technology capable of both comprehending and generating human language, boasting versatile applications across diverse industries. It has ascended to a position of prominence, surpassing its predecessor, LLaMA, another substantial language model, and is now widely regarded as the preeminent leader within the realm of open-source LLMs. |
| [LLaMA 2](https://ai.meta.com/llama/)               | [Meta AI](https://ai.meta.com/)                  | [LLaMa2-7B](https://huggingface.co/meta-llama/Llama-2-7b), [LLaMa2-13B](https://huggingface.co/meta-llama/Llama-2-13b-hf), [LLaMa2-70B](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) | Designed to help researchers & developers in the field of AI by providing a versatile and efficient LLM, trained on a wide range of text sources, including Common Crawl, GitHub, Wikipedia, Project Gutenberg, ArXiv, and Stack Exchange.                                                                                                                                                          |
| [MPT](https://www.mosaicml.com/mpt)                 | [Mosaic ML](https://www.mosaicml.com/)           | [MPT-7B](https://huggingface.co/mosaicml/mpt-7b)                                                                                                                                               | It is a decoder-style transformer that has been pretrained from scratch on 1T tokens of English text and code, available for commercial use, Fast training and inference, Handling long inputs.                                                                                                                                                                                                     |
| [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) | [LMSYS ORG](https://lmsys.org/)                  | [Vicuna-7B](https://huggingface.co/lmsys/vicuna-7b-v1.5), [Vicuna-13B](https://huggingface.co/lmsys/vicuna-13b-v1.5-16k), [Vicuna-33B](https://huggingface.co/lmsys/vicuna-33b-v1.3)           | Vicuna is an open-source chatbot that has undergone fine-tuning using user-shared conversations gathered from ShareGPT. In initial assessments, where GPT-4 served as the evaluator, Vicuna-13B demonstrated a quality level of over 90%, surpassing both OpenAI ChatGPT and Google Bard.                                                                                                           |
| [Bloom](https://huggingface.co/bigscience/bloom)    | [BigScience](https://bigscience.huggingface.co/) | [Bloom-176B](https://huggingface.co/bigscience/bloom)                                                                                                                                          | An open-source multilingual LLM that can generate text in 46 languages and 13 programming languages. It is the first multilingual LLM trained by more than 1,000 AI researchers, available for research and commercial purposes.                                                                                                                                                                    |
| [LLaVA](https://llava-vl.github.io/)                | Haotian Liu                                      | [LLaVA-7B](https://huggingface.co/liuhaotian/llava-v1.5-7b), [LLaVA-13B](https://huggingface.co/liuhaotian/llava-v1.5-13b)                                                                     | LLaVA-1.5 excels in 11 benchmarks with minimal changes from LLaVA, using only public data, quick training in ~1 day on a single 8-A100 node, and outperforming billion-scale data methods. It's a powerful multimodal model, rivaling GPT-4 in chat capabilities and setting a new Science QA accuracy record.                                                                                      |

<Callout emoji="⚠️">
  In this swiftly advancing field, although new LLMs may not appear consistently
  within days, our commitment to timely updates endures.
</Callout>
