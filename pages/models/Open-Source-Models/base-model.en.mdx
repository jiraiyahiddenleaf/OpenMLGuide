


# Base Models

Within the domain of LLMs, the term "base model" denotes the initial iteration, meticulously honed through extensive training on extensive datasets utilizing deep learning methodologies. Following this foundational training, it can be tailored to execute specific tasks, such as text classification, question-answering, summarization, and text generation. Base model constitutes the fundamental framework underpinning practical AI applications.

![Open-source LLM img](/static/img/osLLm.png)

### Models

| Models                                              | Developed by                                     | Parameter                                                                                                                                                                                      | Description                                                                                                                                                                                                                                                                                                                                                                                         |
| --------------------------------------------------- | ------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Mistral](https://mistral.ai/product/)              | [Mistral AI](https://mistral.ai/)                | [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1), [Mistral-7B-Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)                                               | Mistral-7B-v0.1 is a small, yet powerful model adaptable to many use-cases. It outperforms Llama 2 13B on all benchmark assessments, possesses inherent natural coding capabilities, and 8k sequence length. Released under the Apache 2.0 license, it has been designed for effortless deployment on a wide range of cloud platforms, ensuring accessibility and convenience for users.            |
| [Falcon](https://falconllm.tii.ae/)                 | [TII](https://falconllm.tii.ae/index.html)       | [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b), [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b), [Falcon-180B](https://huggingface.co/tiiuae/falcon-180B)                         | The Falcon LLM has garnered acclaim as an advanced technology capable of both comprehending and generating human language, boasting versatile applications across diverse industries. It has ascended to a position of prominence, surpassing its predecessor, LLaMA, another substantial language model, and is now widely regarded as the preeminent leader within the realm of open-source LLMs. |
| [LLaMA 2](https://ai.meta.com/llama/)               | [Meta AI](https://ai.meta.com/)                  | [LLaMa2-7B](https://huggingface.co/meta-llama/Llama-2-7b), [LLaMa2-13B](https://huggingface.co/meta-llama/Llama-2-13b-hf), [LLaMa2-70B](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) | Designed to help researchers & developers in the field of AI by providing a versatile and efficient LLM, trained on a wide range of text sources, including Common Crawl, GitHub, Wikipedia, Project Gutenberg, ArXiv, and Stack Exchange.                                                                                                                                                          |
| [MPT](https://www.mosaicml.com/mpt)                 | [Mosaic ML](https://www.mosaicml.com/)           | [MPT-7B](https://huggingface.co/mosaicml/mpt-7b)                                                                                                                                               | It is a decoder-style transformer that has been pretrained from scratch on 1T tokens of English text and code, available for commercial use, Fast training and inference, Handling long inputs.                                                                                                                                                                                                     |
| [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) | [LMSYS ORG](https://lmsys.org/)                  | [Vicuna-7B](https://huggingface.co/lmsys/vicuna-7b-v1.5), [Vicuna-13B](https://huggingface.co/lmsys/vicuna-13b-v1.5-16k), [Vicuna-33B](https://huggingface.co/lmsys/vicuna-33b-v1.3)           | Vicuna is an open-source chatbot that has undergone fine-tuning using user-shared conversations gathered from ShareGPT. In initial assessments, where GPT-4 served as the evaluator, Vicuna-13B demonstrated a quality level of over 90%, surpassing both OpenAI ChatGPT and Google Bard.                                                                                                           |
| [Bloom](https://huggingface.co/bigscience/bloom)    | [BigScience](https://bigscience.huggingface.co/) | [Bloom-176B](https://huggingface.co/bigscience/bloom)                                                                                                                                          | An open-source multilingual LLM that can generate text in 46 languages and 13 programming languages. It is the first multilingual LLM trained by more than 1,000 AI researchers, available for research and commercial purposes.                                                                                                                                                                    |
| [Fuyu](https://www.adept.ai/blog/fuyu-8b)           | [ADEPT](https://www.adept.ai/)                   | [Fuyu-8b](https://huggingface.co/adept/fuyu-8b)                                                                                                                                                | Fuyu-8B, an ADEPT multimodal model, excels at text and image comprehension. It simplifies the traditional transformer architecture, making it easier to grasp, scale, and deploy. Fuyu-8B handles complex visual relationships, including charts and documents, and performs tasks like OCR and text localization in images.                                                                        |
