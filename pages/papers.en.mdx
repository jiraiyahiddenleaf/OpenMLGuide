# Research Papers

Explore an ever-evolving repository of AI wisdom, showcasing the latest, most influential, and historically significant papers, thoughtfully organized by their publication dates. Our weekly updates guarantee access to the forefront of AI research.

![research paper meme](/static/img/memes/r&dMeme.png)

<br />

- [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560) (Oct 2023)
- [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://arxiv.org/abs//2310.06117) (Oct 2023)
- [Introducing The Foundation Model Transparency Index](https://hai.stanford.edu/news/introducing-foundation-model-transparency-index)
- [Improving Image Generation with Better Captions](https://cdn.openai.com/papers/dall-e-3.pdf)
- [Habitat 3.0: A Co-Habitat for Humans, Avatars and RobotsHabitat 3.0: A Co-Habitat for Humans, Avatars and Robots](https://aihabitat.org/habitat3/)
- [Self-RAG: Learning to Retrieve, Generate and Critique through Self-Reflections](https://selfrag.github.io/) (Oct 2023)
- [Improved Baselines with Visual Instruction Tuning](https://browse.arxiv.org/pdf/2310.03744.pdf) (Oct 2023)
- [Open X-Embodiment: Robotic Learning Datasets and RT-X Models](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/scaling-up-learning-across-many-different-robot-types/Open_X_Embodiment__Robotic_Learning_Datasets_and_RT_X_Models.pdf) (Oct 2023)
- [Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation](https://arxiv.org/pdf/2310.02304.pdf) (Oct 2023)
- [RealFill: Reference-Driven Generation for Authentic Image Completion](https://arxiv.org/abs/2309.16668) (Sept 2023)

import { Callout } from "nextra-theme-docs";

<br />
<Callout emoji="⚠️">This page is under development.</Callout>

### How to Read Research Papers

- [How to Read Research Papers: A Pragmatic Approach for ML Practitioners](https://developer.nvidia.com/blog/how-to-read-research-papers-a-pragmatic-approach-for-ml-practitioners/): This post offers steps for effectively reading machine learning research papers: topic selection, skimming title, abstract, conclusion, delving into the introduction, examining figures, deep reading, and clarifying unfamiliar terms. A valuable resource for newcomers.

### Where to Read Research Papers 

- [Pretrain, Prompt, Predict](http://pretrain.nlpedia.ai/): The website offers a prompt learning timeline, paper list, and GitHub repository. It's a valuable resource for those interested in exploring research papers on pre-training and prompt-based learning in NLP.

- [Papers with Code](https://paperswithcode.com/): The website compiles research papers in machine learning and AI, offering accompanying code. It features a search function, with each paper providing a summary, paper link, and code link for implementation.

- [arXiv](https://arxiv.org/) is an open-source, open-access repository that plays a crucial role in the machine learning community. It hosts electronic preprints and postprints, providing a wealth of papers and resources for staying informed and sharing knowledge in the field.

### Reference

- [arXiv-sanity lite](https://arxiv-sanity-lite.com/): Users can tag papers of interest, receiving recommendations for each tag using SVMs over tf-idf features from paper abstracts. It offers search, ranking, sorting, and customization in a user-friendly web interface. Lastly, arxiv-sanity-lite provides daily email recommendations based on your tags. [(code)](https://github.com/karpathy/arxiv-sanity-lite)
