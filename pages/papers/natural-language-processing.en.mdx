# Natural Language Processing Papers

[Paper with Code NLP papes section](https://paperswithcode.com/area/natural-language-processing): Provides access to research papers along with the corresponding code.


- [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560) (Oct 2023)
- [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://arxiv.org/abs//2310.06117) (Oct 2023)
- [Introducing The Foundation Model Transparency Index](https://hai.stanford.edu/news/introducing-foundation-model-transparency-index)
- [Improving Image Generation with Better Captions](https://cdn.openai.com/papers/dall-e-3.pdf)
- [Habitat 3.0: A Co-Habitat for Humans, Avatars and RobotsHabitat 3.0: A Co-Habitat for Humans, Avatars and Robots](https://aihabitat.org/habitat3/)
- [Self-RAG: Learning to Retrieve, Generate and Critique through Self-Reflections](https://selfrag.github.io/) (Oct 2023)
- [Improved Baselines with Visual Instruction Tuning](https://browse.arxiv.org/pdf/2310.03744.pdf) (Oct 2023)
- [Open X-Embodiment: Robotic Learning Datasets and RT-X Models](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/scaling-up-learning-across-many-different-robot-types/Open_X_Embodiment__Robotic_Learning_Datasets_and_RT_X_Models.pdf) (Oct 2023)
- [Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation](https://arxiv.org/pdf/2310.02304.pdf) (Oct 2023)
- [RealFill: Reference-Driven Generation for Authentic Image Completion](https://arxiv.org/abs/2309.16668) (Sept 2023)
- [GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models](https://arxiv.org/abs/2303.10130) (Aug 2023)


### Language Modelling

- [Generating Sequences With Recurrent Neural Networks](https://arxiv.org/abs/1308.0850v5) (2013)
- [Semi-supervised Sequence Learning](https://arxiv.org/abs/1511.01432v1) (2015)
- [End-To-End Memory Networks](https://arxiv.org/abs/1503.08895v5) (2015)
- [Listen, Attend and Spell](https://arxiv.org/abs/1508.01211v2) (2015)
- [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/abs/1708.02182v1) (2017)
- [Deep contextualized word representations](https://arxiv.org/abs/1802.05365v2) (2018)
- [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146v5) (2018)
- [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271v2) (2018)
- [DARTS: Differentiable Architecture Search](https://arxiv.org/abs/1806.09055v2) (2019)
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692v1) (2019)
- [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860v3) (2019)
- [Well-Read Students Learn Better: On the Importance of Pre-training Compact Models](https://arxiv.org/abs/1908.08962v2) (2019)
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165v4) (2020)

### Text Generation

- [Generating Sequences With Recurrent Neural Networks](https://arxiv.org/abs/1308.0850v5) (2013)
- [Show and Tell: A Neural Image Caption Generator](https://arxiv.org/abs/1411.4555v2) (2014)
- [Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/abs/1610.02424v2) (2016)
- [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473v6) (2017)
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461v1) (2019)
- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020v1) (2021)

### Sentiment Analysis

- [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759v3) (2016)
- [A Structured Self-attentive Sentence Embedding](https://arxiv.org/abs/1703.03130v1) (2017)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805v2) (2018)
- [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146v5) (2018)
- [Deep contextualized word representations](https://arxiv.org/abs/1802.05365v2) (2018)
- [Well-Read Students Learn Better: On the Importance of Pre-training Compact Models](https://arxiv.org/abs/1908.08962v2) (2019)
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692v1) (2019)
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683v4) (2019)

### Text Classification

- [Semi-supervised Sequence Learning](https://arxiv.org/abs/1511.01432v1) (2015)
- [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759v3) (2016)
- [FastText.zip: Compressing text classification models](https://arxiv.org/abs/1612.03651v1) (2016)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805v2) (2018)
- [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146v5) (2018)

### Key Papers

- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) (2013): Introduced two model architectures for word vector representations derived from large datasets, outperforming existing methods in word similarity tasks with enhanced accuracy and lower computational demands. These vectors excel in measuring syntactic and semantic word similarities.

- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) (2014): Proposed GloVe, a model that learns word meanings from co-occurrence statistics. It uses a global co-occurrence matrix to derive word vectors, demonstrating superior performance in various word analogy and similarity tasks compared to other methods.

- [Deep contextualized word representations](https://arxiv.org/abs/1802.05365) (2018): Introduces novel word representations capturing both meaning and context in sentences. These representations stem from a deep bidirectional language model trained on extensive text. They excel in various NLP tasks, such as sentiment analysis and named entity recognition, surpassing other techniques in performance.

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (2018): Presents BERT, a pre-trained deep bidirectional transformer model. Trained on extensive text data with a masked language modeling objective, it excels in natural NLP like question answering and sentiment analysis, surpassing alternative methods in performance and versatility.

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017): Introduced the groundbreaking concept of the Transformer architecture, an attention-based neural architecture for sequence processing. It outperforms other methods in NLP tasks like machine translation and language modeling, demonstrating its effectiveness in capturing contextual information from input sequences like sentences. [(blog)](https://blog.research.google/2017/08/transformer-novel-neural-network.html)

- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) (2014): Introduces the sequence-to-sequence model, a neural network for tasks like machine translation and text summarization. Comprising an encoder and a decoder, it effectively processes input sequences and generates output sequences, outperforming other methods in a range of NLP tasks.

- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401v4) (2020): Introduces a versatile fine-tuning method for retrieval-augmented generation (RAG) models, combining parametric and non-parametric memory for language generation. They employ a pre-trained neural retriever to fetch Wikipedia passages for input, achieving state-of-the-art results on knowledge-intensive NLP tasks. RAG models offer more precise, diverse, and factual language generation. [(blog)](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/)