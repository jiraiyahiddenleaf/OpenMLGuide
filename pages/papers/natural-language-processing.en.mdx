# Natural Language Processing Papers

[Paper with Code NLP papes section](https://paperswithcode.com/area/natural-language-processing): Provides access to research papers along with the corresponding code.


- [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560) (Oct 2023)
- [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://arxiv.org/abs//2310.06117) (Oct 2023)
- [Introducing The Foundation Model Transparency Index](https://hai.stanford.edu/news/introducing-foundation-model-transparency-index)
- [Improving Image Generation with Better Captions](https://cdn.openai.com/papers/dall-e-3.pdf)
- [Habitat 3.0: A Co-Habitat for Humans, Avatars and RobotsHabitat 3.0: A Co-Habitat for Humans, Avatars and Robots](https://aihabitat.org/habitat3/)
- [Self-RAG: Learning to Retrieve, Generate and Critique through Self-Reflections](https://selfrag.github.io/) (Oct 2023)
- [Improved Baselines with Visual Instruction Tuning](https://browse.arxiv.org/pdf/2310.03744.pdf) (Oct 2023)
- [Open X-Embodiment: Robotic Learning Datasets and RT-X Models](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/scaling-up-learning-across-many-different-robot-types/Open_X_Embodiment__Robotic_Learning_Datasets_and_RT_X_Models.pdf) (Oct 2023)
- [Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation](https://arxiv.org/pdf/2310.02304.pdf) (Oct 2023)
- [RealFill: Reference-Driven Generation for Authentic Image Completion](https://arxiv.org/abs/2309.16668) (Sept 2023)
- [GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models](https://arxiv.org/abs/2303.10130) (2023): Anticipates 80% of U.S. workers with 10% task impact, and 19% with 50% task impact due to LLM introduction.

- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) (2013): Introduced two model architectures for word vector representations derived from large datasets, outperforming existing methods in word similarity tasks with enhanced accuracy and lower computational demands. These vectors excel in measuring syntactic and semantic word similarities.

- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) (2014): Proposed GloVe, a model that learns word meanings from co-occurrence statistics. It uses a global co-occurrence matrix to derive word vectors, demonstrating superior performance in various word analogy and similarity tasks compared to other methods.

- [Deep contextualized word representations](https://arxiv.org/abs/1802.05365) (2018): Introduces novel word representations capturing both meaning and context in sentences. These representations stem from a deep bidirectional language model trained on extensive text. They excel in various NLP tasks, such as sentiment analysis and named entity recognition, surpassing other techniques in performance.

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (2018): Presents BERT, a pre-trained deep bidirectional transformer model. Trained on extensive text data with a masked language modeling objective, it excels in natural NLP like question answering and sentiment analysis, surpassing alternative methods in performance and versatility.

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017): Introduced the groundbreaking concept of the Transformer architecture, an attention-based neural architecture for sequence processing. It outperforms other methods in NLP tasks like machine translation and language modeling, demonstrating its effectiveness in capturing contextual information from input sequences like sentences. [(blog)](https://blog.research.google/2017/08/transformer-novel-neural-network.html)

- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) (2014): Introduces the sequence-to-sequence model, a neural network for tasks like machine translation and text summarization. Comprising an encoder and a decoder, it effectively processes input sequences and generates output sequences, outperforming other methods in a range of NLP tasks.

- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401v4) (2020): Introduces a versatile fine-tuning method for retrieval-augmented generation (RAG) models, combining parametric and non-parametric memory for language generation. They employ a pre-trained neural retriever to fetch Wikipedia passages for input, achieving state-of-the-art results on knowledge-intensive NLP tasks. RAG models offer more precise, diverse, and factual language generation. [(blog)](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/)