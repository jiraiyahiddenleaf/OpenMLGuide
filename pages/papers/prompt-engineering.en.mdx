# Prompt Engineering Papers

[Paper with Code prompt engineering section](https://paperswithcode.com/task/prompt-engineering):  Provides access to research papers along with the corresponding code.

## Overview

- [OpenPrompt: An Open-source Framework for Prompt-learning](https://arxiv.org/abs/2111.01998) (2021): Introduces a flexible framework for prompt-based learning that allows prompting and finetuning large PLMs like BERT, GPT-2, T5 etc. Provides implementations for prompt engineering and analysis. [(project)](https://github.com/thunlp/OpenPrompt)

- [Pre-Trained Models: Past, Present and Future](https://arxiv.org/abs/2106.07139) (2021): Provides a historical overview and taxonomy of foundation models like BERT. Analyzes tradeoffs in model scaling, data, compute, and transferability.

- [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/abs/2107.13586) (2021): Surveys different prompting formulations for NLP including soft prompts, hard prompts, continuous prompts etc. Analyzes prompt tuning objectives and benchmarks performance. [(project)](http://pretrain.nlpedia.ai/)

- [Paradigm Shift in Natural Language Processing](https://arxiv.org/abs/2109.12575) (2021): Discusses the shift from feature engineering to pretraining large neural models on unlabeled text. Transfer learning has driven progress on many NLP tasks. [(project)](https://txsun1997.github.io/nlp-paradigm-shift/)

## Pilot Work

- [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751) (2019): Introduces methods to reduce sizes of pretrained models by pruning and distillation to improve parameter efficiency of transfer learning. [(project)](https://github.com/google-research/adapter-bert)

- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) (2019): Proposes T5 model pretrained on a unified text-to-text format. Shows strong transfer learning performance on diverse NLP tasks. [(project)](https://github.com/google-research/text-to-text-transfer-transformer)

- [Language Models as Knowledge Bases?](https://arxiv.org/abs/1909.01066) (2019): Investigates using language models like BERT for knowledge base completion and fact retrieval by querying the model. [(project)](https://github.com/facebookresearch/LAMA)

- [How Can We Know What Language Models Know?](https://arxiv.org/abs/1911.12543) (2019): Analyzes methods to probe linguistic knowledge learned by language models, testing capabilities like coreference resolution. [(project)](https://github.com/jzbjyb/LPAQA)

- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (2019): Shows LMs can perform well on many NLP tasks with only a small number of training examples. [(blog)](https://openai.com/blog/gpt-3-apps/)

- [AdaPrompt: Adaptive Model Training for Prompt-based NLP](https://arxiv.org/abs/2202.04824) (2022): Proposes an adaptive prompting method that automatically searches over prompt space during model tuning.

## Basics

- [Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference](https://arxiv.org/abs/2001.07676) (2020): Uses cloze-style prompts with masked tokens for few-shot text classification and NLI tasks. [(project)](https://github.com/timoschick/pet)

- [It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners](https://arxiv.org/abs/2009.07118) (2020): Shows even small pretrained language models can achieve good performance on few-shot NLP tasks through prompt tuning. [(project)](https://github.com/timoschick/pet)

- [AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://arxiv.org/abs/2010.15980) (2020): Automatically generates prompt templates for querying knowledge from LMs without manual engineering. [(website)](https://ucinlp.github.io/autoprompt/)

- [Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification](https://arxiv.org/abs/2010.13641) (2020): Proposes methods to automatically identify words that can serve as class labels for few-shot text classification. [(project)](https://github.com/timoschick/pet)

- [Making Pre-trained Language Models Better Few-shot Learners](https://arxiv.org/abs/2012.15723) (2021): Introduces new pretraining objectives like masked language modeling to better adapt LMs for few-shot fine-tuning. [(project)](https://github.com/princeton-nlp/LM-BFF)

- [Prefix-tuning: Optimizing continuous prompts for generation](https://arxiv.org/abs/2101.00190) (2021): Introduces continuous prompt tuning approach with trainable prefixes appended to text sequences. [(project)](https://github.com/XiangLi1999/PrefixTuning)

- [Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm](https://arxiv.org/abs/2102.07350) (2021): Provides a programming framework for specifying prompts to perform complex reasoning tasks.

- [Improving and Simplifying Pattern Exploiting Training](https://arxiv.org/abs/2103.11955) (2021): Enhances pattern exploiting training with additional supervisory signals and modeling advances.

- [GPT understands, too](https://arxiv.org/abs/2103.10385) (2021): Demonstrates that GPT models can perform reasoning tasks when prompted with appropriate formulations. [(project)](https://github.com/THUDM/P-tuning)

- [The Power of Scale for Parameter-Efﬁcient Prompt Tuning](https://arxiv.org/abs/2104.08691) (2021): Shows that larger pretrained language models better leverage soft prompt tuning across NLP tasks. [(project)](https://github.com/kipgparker/soft-prompt-tuning)

- [Learning How to Ask: Querying LMs with Mixtures of Soft Prompts](https://arxiv.org/abs/2104.06599) (2021): Proposes prompting with weighted mixtures of soft prompt templates. [(project)](https://github.com/hiaoxui/soft-prompts)

- [Factual Probing Is [MASK]: Learning vs. Learning to Recall](https://arxiv.org/abs/2104.05240) (2021): Studies how much factual knowledge is retained in the parameters of language models. [(project)](https://github.com/princeton-nlp/OptiPrompt)

- [Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models](https://arxiv.org/abs/2106.13353) (2021): Achieves strong few-shot performance with simple prompt tuning approaches without complex formulations.

- [WARP: Word-level Adversarial ReProgramming](https://arxiv.org/abs/2101.00121) (2021): Adversarially generates prompts to reprogram undesirable behaviors in LMs. [(project)](https://github.com/YerevaNN/WARP)

- [PTR: Prompt Tuning with Rules for Text Classification](https://arxiv.org/abs/2105.11259) (2021): Incorporates human-provided rules to improve prompting for text classification.

- [NSP-BERT: A Prompt-based Few-Shot Learner Through an Original Pre-training Task--Next Sentence Prediction](https://arxiv.org/abs/2109.03564) (2021): Pretrains BERT model using next sentence prediction as a self-supervised task. [(project)](https://github.com/sunyilgdx/NSP-BERT)

- [Finetuned language models are zero-shot learners](https://arxiv.org/abs/2109.01652) (2021): Shows finetuned LMs can perform well on unseen tasks with no gradient updates.

- [PPT: Pre-trained Prompt Tuning for Few-shot Learning](https://arxiv.org/abs/2109.04332) (2021): Pretrains prompts on masked language modeling as initialization for few-shot tuning.

- [Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners](https://arxiv.org/abs/2108.13161) (2021): Makes prompts differentiable end-to-end for gradient-based optimization. [(project)](https://github.com/zjunlp/DART)

- [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207) (2021): Jointly trains prompts across multiple NLP tasks.

- [P-Tuning v2: Prompt Tuning Can Be Comparable to Finetuning Universally Across Scales and Tasks](https://arxiv.org/abs/2110.07602) (2021): Shows prompt tuning can approach finetuning performance with large LMs. [(project)](https://github.com/THUDM/P-tuning-v2)

- [Black-Box Tuning for Language-Model-as-a-Service](https://arxiv.org/abs/2201.03514) (2022): Enables querying LMs without access to gradients or parameters. [(project)](https://github.com/txsun1997/Black-Box-Tuning)

- [Black-box Prompt Learning for Pre-trained Language Models](https://arxiv.org/abs/2201.08531) (2022): Learns prompts by maximizing probability of target texts.

- [Binding Language Models in Symbolic Languages](https://arxiv.org/abs/2210.02875) (2022): Binds parameters of LMs to symbols to induce reasoning. [(project)](https://github.com/HKUNLP/Binder) [(website)](https://lm-code-binder.github.io/)

- [A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT](https://arxiv.org/abs/2302.11382) (2023): Curates prompt patterns to enhance prompt engineering.

## Analysis

- [What Makes Good In-Context Examples for GPT-3?](https://arxiv.org/abs/2101.06804) (2021)

- [How Many Data Points is a Prompt Worth?](https://arxiv.org/abs/2103.08493) (2021)
  [(project)](https://github.com/TevenLeScao/pet)

- [Surface Form Competition-Why the Highest Probability Answer Isn’t Always Right](https://arxiv.org/abs/2104.08315) (2021) [(project)](https://github.com/peterwestuw/surface-form-competition)

- [Natural Instructions: Benchmarking Generalization to New Tasks from Natural Language Instructions](https://arxiv.org/abs/2104.08773) (2021) [(project)](https://arxiv.org/abs/2104.08773)

- [Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity](https://arxiv.org/abs/2104.08786) (2021)

- [Meta-tuning Language Models to Answer Prompts Better](https://arxiv.org/abs/2104.04670) (2021)

- [True Few-Shot Learning with Language Models](https://arxiv.org/abs/2105.11447) (2021) [(project)](https://github.com/ethanjperez/true_few_shot)

- [Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning](https://arxiv.org/abs/2106.09226) (2021)

- [Do Prompt-Based Models Really Understand the Meaning of their Prompts?](https://arxiv.org/abs/2109.01247) [(project)](https://github.com/awebson/prompt_semantics)

- [Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning](https://arxiv.org/abs/2109.04144) (2021)

- [Towards a Unified View of Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2110.04366) (2021)

- [Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning](https://arxiv.org/abs/2110.07867) (2022)

- [Exploring the Universal Vulnerability of Prompt-based Learning Paradigm. Findings of NAACL](https://arxiv.org/abs/2204.05239) (2022) [(website)](https://github.com/leix28/prompt-universal-vulnerability)

- [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837) (2022) [(project)](https://github.com/Alrope123/rethinking-demonstrations)

- [Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated Neural Text Retrievers](https://arxiv.org/abs/2207.07087) (2022) [(project)](https://github.com/THUDM/P-tuning-v2/tree/main/PT-Retrieval)

- [Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527) (2022) [(project)](https://github.com/agencyenterprise/PromptInject)

## Improvements

- [Calibrate Before Use: Improving Few-Shot Performance of Language Models](https://arxiv.org/abs/2102.09690) (2021) [(project)](https://github.com/tonyzhaozh/few-shot-learning)

- [Text Generation with Efficient (Soft) Q-Learning](https://arxiv.org/abs/2106.07704) (2021)

- [Noisy Channel Language Model Prompting for Few-Shot Text Classiﬁcation](https://arxiv.org/abs/2108.04106) (2021)

- [Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification](https://arxiv.org/abs/2108.02035) (2021) [(project)](https://github.com/ShengdingHu/KnowledgeablePromptTuning)

- [Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collection](http://nlp.cs.berkeley.edu/pubs/Zhong-Lee-Zhang-Klein_2021_MetaTuning_paper.pdf) (2021)

- [Revisiting Self-Training for Few-Shot Learning of Language Model](https://arxiv.org/abs/2110.01256) (2021)

- [LiST: Lite Self-training Makes Efficient Few-shot Learners](https://arxiv.org/abs/2110.06274) (2021)

- [Prototypical Verbalizer for Prompt-based Few-shot Tuning](https://arxiv.org/abs/2203.09770) (2022) [(project)](https://github.com/thunlp/OpenPrompt)

- [BBTv2: Pure Black-Box Optimization Can Be Comparable to Gradient Descent for Few-Shot Learning](https://arxiv.org/abs/2205.11200) (2022) [(project)](https://github.com/txsun1997/Black-Box-Tuning)

## Specializations

- [Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2](https://arxiv.org/abs/2103.13033) (2021)

- [GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation](https://arxiv.org/abs/2104.08826) (2021)

- [Constrained Language Models Yield Few-Shot Semantic Parsers](https://arxiv.org/abs/2110.07867) (2022)

- [Label Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction](https://arxiv.org/abs/2109.03659) (2021)

- [PADA: A Prompt-based Autoregressive Approach for Adaptation to Unseen Domains](https://arxiv.org/abs/2102.12206) (2021) [(project)](https://github.com/eyalbd2/PADA)

- [Prompt-Learning for Fine-grained Entity Typing](https://arxiv.org/abs/2108.10604) (2021)

- [KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction](https://arxiv.org/abs/2104.07650) (2021) [(project)](https://github.com/zjunlp/KnowPrompt)

- [Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation](https://arxiv.org/abs/2109.06513) (2021)

- [SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2109.08306) (2021)

- [Template-free Prompt Tuning for Few-shot NER](https://arxiv.org/abs/2109.13532) (2021)

- [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134) (2021)

- [CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models](https://arxiv.org/abs/2109.11797) (2021)

- [MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators](https://arxiv.org/abs/2110.06609) (2021)

- [Few-Shot Bot: Prompt-Based Learning for Dialogue Systems](https://arxiv.org/abs/2110.08118) (2021)

- [Control Prefixes for Text Generation](https://arxiv.org/abs/2110.08329) (2021)

- [The Power of Prompt Tuning for Low-Resource Semantic Parsing](https://arxiv.org/abs/2110.08525) (2021)

- [A Good Prompt Is Worth Millions of Parameters? Low-resource Prompt-based Learning for Vision-Language Models](https://arxiv.org/abs/2110.08484) (2021)

- [LightNER: A Lightweight Generative Framework with Prompt-guided Attention for Low-resource NER](https://arxiv.org/abs/2109.00720) (2021)

- [UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models](https://arxiv.org/abs/2201.05966) (2022) [(website)](https://unifiedskg.com/) [(project)](https://github.com/hkunlp/unifiedskg)

- [Ontology-enhanced Prompt-tuning for Few-shot Learning](https://arxiv.org/abs/2201.11332) (2022)

- [Learning to Prompt for Continual Learning](https://arxiv.org/abs/2112.08654) (2021) [(project)](https://github.com/google-research/l2p)

- [Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning](https://arxiv.org/abs/2205.02355) (2022) [(project)](https://github.com/zjunlp/PromptKG/tree/main/research/RetrievalRE)

- [Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction](https://arxiv.org/abs/2205.03521) (2022)
  [(website)](https://github.com/zjunlp/HVPNeT)

- [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) (2022)

- [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171) (2022)

- [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916) (2022)

- [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625) (2022)

- [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822) (2022)

- [On the Advance of Making Language Models Better Reasoners](https://arxiv.org/abs/2205.11822) (2022)

- [Learning to Compose Soft Prompts for Compositional Zero-Shot Learning](https://arxiv.org/abs/2204.03574) (2022) [(project)](https://github.com/BatsResearch/csp)

- [Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning](ttps://arxiv.org/abs/2205.14704) (2022) [(project)](https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt)

- [Exploring Length Generalization in Large Language Models](https://arxiv.org/abs/2207.04901) (2022)

- [Ask Me Anything: A simple strategy for prompting language models](https://arxiv.org/abs/2210.02441) (2022)

- [Measuring And Narrowing The Compositionality Gap In Language Models](https://ofir.io/self-ask.pdf) (2022)

- [RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning](https://arxiv.org/abs/2205.12548) (2022)

- [Reasoning with Language Model Prompting: A Survey](https://arxiv.org/abs/2212.09597) (2022)
