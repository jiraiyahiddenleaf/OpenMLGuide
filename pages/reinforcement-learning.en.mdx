# Reinforcement Learning

Reinforcement Learning (RL) is an AI paradigm where agents learn by interacting with their environment. It mimics trial-and-error learning, aiming to maximize cumulative rewards. Deep Dive into Deep Reinforcement Learning (DRL) delves further into RL, exploring how neural networks enhance its capabilities. This subfield pioneers self-improving algorithms and their applications, from robotics to gaming.

<br />
import Image from 'next/image'
import myGif from 'public/gif/rl.gif'

<Image src={myGif} alt="my gif" />

### Courses

- [Introduction to Reinforcement Learning](https://www.youtube.com/playlist?list=PLFihX_3MLxS-xlTJeDMGUMKpx12fFR4jw) by David Silver. It covers key Reinforcement Learning methods, including [AlphaZero's](https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go) success in chess, Go, and Shogi.

- [Reinforcement Learning Lecture Series 2021](https://www.youtube.com/playlist?list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm) by DeepMind in collaboration with University College London, this series encompasses fundamental concepts, Markov Decision Processes, sample-based learning methods including Q-learning, deep reinforcement learning, and advanced subjects such as off-policy learning and the practical implementation of deep reinforcement learning algorithms like rainbow DQN. It provides a thorough grasp of reinforcement learning and its applications.

- [Deep Reinforcement Learning Course](https://huggingface.co/learn/deep-rl-course/unit0/introduction): Hugging Face offers a comprehensive DRL course suitable for beginners to experts. It provides theoretical and practical knowledge, including hands-on exercises using popular libraries like [Stable Baselines3](https://stable-baselines3.readthedocs.io/en/master/) and [RL Baselines3 Zoo](https://stable-baselines3.readthedocs.io/en/master/guide/rl_zoo.html). The course offers unique environments and is divided into units, starting with foundational topics, such as training a lunar lander on the Moon. Prerequisites include basic Python, linear algebra, and calculus knowledge.

### Explainers

- [MIT 6.S094: Deep Reinforcement Learning for Motion Planning](https://www.youtube.com/watch?v=QDzM8r3WgBw&list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf) by Lex Fridman, introduces types of machine learning, the neuron as a computational building block for neural nets, Q-learning, deep reinforcement learning, and the DeepTraffic simulation that utilizes deep reinforcement learning for the motion planning task.

- [MIT 6.S191: Reinforcement Learning](https://www.youtube.com/watch?v=AhyznRSDjw8) by Alexander Amini, covers the basics of Reinforcement Learning, including Markov Decision Processes, Value Iteration, and Q-Learning.

- [AlphaGo - How AI mastered the hardest boardgame in history](https://www.youtube.com/watch?v=MgowR4pq3e8&t=503s) by Arxiv Insights, focuses on AlphaGo, an AI by Google's DeepMind, defeating the Go world champion. It highlights AI's progress and significance, mentions system limitations, and emphasizes the need for further research to unlock AI's potential.

### Articles

- [Deep Learning in a Nutshell: Reinforcement Learning](https://developer.nvidia.com/blog/deep-learning-nutshell-reinforcement-learning/) by Nvidia offering an intuitive introduction to Reinforcement Learning. It covers key concepts, value and policy functions, and uses analogies and images for clarity.

- [AlphaGo](https://www.deepmind.com/research/highlighted-research/alphago) (2015), developed by DeepMind, is a remarkable computer program that masters the game of Go using neural networks. Through reinforcement learning, it trained by playing against itself, continuously improving. AlphaGo defeated world champions in different global arenas, arguably becoming the greatest Go player of all time. It also outperformed itself with [AlphaGo Zero](https://www.deepmind.com/blog/alphago-zero-starting-from-scratch) (2017).

- [AlphaZero](https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go) (2018), developed by DeepMind, autonomously mastered chess, shogi, and Go, surpassing world-champion programs. It combines advanced search trees and neural networks. Despite starting from random play and having no prior knowledge, it excels and exhibits a dynamic, creative playstyle in these games.

- [MuZero: Mastering Go, chess, shogi and Atari without rules](https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules) (2020), developed by DeepMind, excels at Go, chess, shogi, and Atari games without prior knowledge of the rules. It blends AlphaZero's planning with model-free reinforcement learning, predicting relevant future aspects. Setting new benchmarks in reinforcement learning, MuZero matches AlphaZero's superhuman performance.

- [Key Papers in Deep RL](https://spinningup.openai.com/en/latest/spinningup/keypapers.html) is a part of OpenAI's Spinning Up in Deep RL project, offers a categorized list of essential papers in deep reinforcement learning. It provides brief descriptions and algorithm references. Maintained by OpenAI.

### Papers 

- [Q-learning](https://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf) (1992): Introduced an off-policy reinforcement learning algorithm where agents learn to maximize reward through action value estimates, enabling agents to determine optimal behavior for Markov decision processes. This algorithm is one of the fundamental RL methods.

- [Policy invariance under reward transformations: Theory and application to reward shaping](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf) (1999): explores reward shaping in RL, aiming to encourage desired behavior in agents. It introduces the policy invariance concept and outlines a method for reward function modification to achieve this goal.

- [Learning to Predict by the Methods of Temporal Differences](https://www.researchgate.net/publication/225264698_Learning_to_Predict_by_the_Method_of_Temporal_Differences) (1988):  Presents temporal difference learning, a model-free reinforcement learning algorithm for state value prediction. It updates value estimates based on the discrepancy between predicted and actual rewards.

- [Actor-Critic Algorithms](https://proceedings.neurips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf) (2003): Introduced actor-critic algorithms, uniting policy-based and value-based methods. The actor learns state-action policies, while the critic learns value functions. Various algorithm variants were proposed, including the advantage actor-critic and natural actor-critic.

- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) (2017): Presented an actor-critic algorithm for stable deep reinforcement learning using clipped surrogate objectives for the policy update and adaptive KL penalty coefficients for reliable performance. This algorithm enables efficient training of policies in complex environments.

### Guides

- [Awesome Reinforcement Learning](https://github.com/aikorea/awesome-rl) compiles resources on reinforcement learning, encompassing theory, applications, codes, papers, books, tutorials, and open-source platforms. 

- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/): An OpenAI educational resource, simplifies deep reinforcement learning exploration. Tailored for aspiring researchers, it introduces RL fundamentals and various algorithm types. The material delves into specific algorithms, such as TRPO, PPO, and DQN, providing an invaluable foundation for understanding advanced AI applications in today's world.

### Books

- [Grokking Deep Reinforcement Learning](https://www.goodreads.com/en/book/show/50336343) by Miguel Morales melds annotated Python code with lucid explanations to delve into Deep Reinforcement Learning (DRL) techniques. It offers insight into algorithm operations and guides you in creating your own DRL agents through evaluative feedback.


### Reference

- [Stable Baselines3](https://stable-baselines3.readthedocs.io/en/master/index.html) documentation by [DLR-RM](https://rm.dlr.de/) provides a comprehensive overview of this open source Python library for developing and evaluating reinforcement learning algorithms. It covers installation, tutorials, example use cases, customization of policies and algorithms, development tips, and benchmarks. The docs enable users to effectively leverage Stable Baselines3 for RL research and applications.

- [Gym Library](https://www.gymlibrary.dev/) offers an expansive catalog of over 1.8K OpenAI Gym environments for reinforcement learning research across diverse domains including [Atari](https://atari.com/), [Box2d](https://box2d.org/), [MuJoCo](https://mujoco.org/), and more. This well-organized interface enables seamless discovery, comparison, and integration of Gym environments into machine learning projects, empowering faster prototyping, benchmarking, and productive reinforcement learning research. (Consider [Gymnasium](https://gymnasium.farama.org/) for simple interface)

- [PettingZoo](https://pettingzoo.farama.org/) is a simple, pythonic interface capable of representing general multi-agent reinforcement learning (MARL) problems, includes a wide variety of reference environments, helpful utilities, and tools for creating your own custom environments.

- [Minari](https://minari.farama.org/) is a Python API, hosts Offline Reinforcement Learning datasets compatible with the Gymnasium API. Publicly accessible on a [Farama GCP bucket](https://console.cloud.google.com/storage/browser/minari-datasets;tab=objects?forceOnBucketsSortingFiltering=false&amp;project=mcmes-345620&amp;prefix=&amp;forceOnObjectsSortingFiltering=false), it offers features like episode sampling, trajectory filtering, and dataset generation.

